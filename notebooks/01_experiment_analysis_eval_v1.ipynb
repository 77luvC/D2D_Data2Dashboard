{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnIeB334pfpJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from insightbench import benchmarks, metrics\n",
        "\n",
        "# Load your analysis result\n",
        "your_analysis = {\n",
        "    \"descriptive\": \"The dataset provides a comprehensive view of customer interactions...\",\n",
        "    \"predictive\": \"Predictive analysis could focus on identifying factors...\",\n",
        "    \"domain_related\": \"In the CRM domain, understanding customer segmentation...\"\n",
        "}\n",
        "\n",
        "# Convert your analysis to the format expected by InsightBench\n",
        "# InsightBench expects a list of insights\n",
        "pred_insights = [\n",
        "    your_analysis[\"descriptive\"],\n",
        "    your_analysis[\"predictive\"],\n",
        "    your_analysis[\"domain_related\"]\n",
        "]\n",
        "\n",
        "# Load the benchmark for comparison\n",
        "# You need to identify which benchmark dataset you're using\n",
        "dataset_id = 1  # Change this to match your dataset\n",
        "benchmark_path = f\"data/notebooks/flag-{dataset_id}.json\"\n",
        "benchmark_data = benchmarks.load_dataset_dict(benchmark_path)\n",
        "gt_insights = benchmark_data[\"insights\"]\n",
        "\n",
        "# Evaluate using different metrics\n",
        "rouge_score, rouge_details = metrics.compute_rouge(pred_insights, gt_insights)\n",
        "geval_score, geval_details = metrics.compute_g_eval_o2m(pred_insights, gt_insights)\n",
        "\n",
        "print(f\"ROUGE-1 Score: {rouge_score}\")\n",
        "print(f\"G-Eval Score: {geval_score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
