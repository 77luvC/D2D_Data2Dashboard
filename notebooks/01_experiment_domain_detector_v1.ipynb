{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 1: Maketing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This script shows how to call `run_domain_detector`\n",
        "# and save / print the results without hitting the\n",
        "# ‚ÄúJSON object must be str‚Äù TypeError.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. Make sure Python can find your agent package / module\n",
        "#    (adapt the path if your repo layout is different)\n",
        "# ------------------------------------------------------------------\n",
        "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
        "sys.path.append(ROOT_DIR)\n",
        "\n",
        "from agents.domain_detector_wrap import run_domain_detector   # noqa: E402"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset from Google Drive: https://drive.google.com/uc?export=download&id=1JhsgpIulCv8Q9NPTZGhrz5-y_RUufMoO\n",
            "Dataset downloaded successfully (3117 rows, 19 columns)\n",
            "Analyzing dataset...\n",
            "Successfully read CSV: 3117 rows, 19 columns\n",
            "Data profile built successfully\n",
            "Starting analysis with max_cycles=5\n",
            "Decision point ‚Äì iteration 1, scores: {'correctness': 4, 'relevance': 3, 'coverage': 3, 'insightfulness': 3, 'novelty': 3}\n",
            "üîÑ Analysis needs improvement (iteration 1)\n",
            "Decision point ‚Äì iteration 2, scores: {'correctness': 4, 'relevance': 3, 'coverage': 3, 'insightfulness': 3, 'novelty': 3}\n",
            "üîÑ Analysis needs improvement (iteration 2)\n",
            "Decision point ‚Äì iteration 3, scores: {'correctness': 4, 'relevance': 3, 'coverage': 3, 'insightfulness': 4, 'novelty': 4}\n",
            "üõë Reached maximum cycles (3), ending execution.\n",
            "\n",
            "Found 1 iterations in history:\n",
            "Error during processing: name 'print_iteration' is not defined\n",
            "Temporary file removed\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    import json\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import tempfile\n",
        "\n",
        "    # Google Drive URL\n",
        "    url = \"https://drive.google.com/uc?export=download&id=1JhsgpIulCv8Q9NPTZGhrz5-y_RUufMoO\"\n",
        "\n",
        "    print(f\"Downloading dataset from Google Drive: {url}\")\n",
        "\n",
        "    try:\n",
        "        # Create a temporary file for the CSV\n",
        "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.csv')\n",
        "        temp_path = temp_file.name\n",
        "        temp_file.close()\n",
        "\n",
        "        # Download directly with pandas\n",
        "        df = pd.read_csv(url)\n",
        "        df.to_csv(temp_path, index=False)\n",
        "\n",
        "        print(f\"Dataset downloaded successfully ({len(df)} rows, {len(df.columns)} columns)\")\n",
        "\n",
        "        # Run the domain detector\n",
        "        print(\"Analyzing dataset...\")\n",
        "        result = run_domain_detector(temp_path)  # or your desired value\n",
        "\n",
        "        # --- print per-iteration history\n",
        "        history = result.get(\"history\", [])\n",
        "        if not history:\n",
        "            print(\"\\nNo iteration history available in the result.\")\n",
        "        else:\n",
        "            print(f\"\\nFound {len(history)} iterations in history:\")\n",
        "            for item in history:\n",
        "                print_iteration(item)  # Print every log, including iteration = 0\n",
        "\n",
        "            # Print the full raw history for debugging\n",
        "            print(\"\\nFull raw history:\")\n",
        "            import pprint\n",
        "            pprint.pprint(history)\n",
        "\n",
        "        # --- summary\n",
        "        analysis = result.get('analysis', {})\n",
        "        domain = analysis.get('domain', 'Unknown')\n",
        "        scores = result.get('scores', {})\n",
        "\n",
        "        print(\"\\n‚úÖ  Final detected domain:\", domain)\n",
        "        print(\"‚úÖ  Score card:\", scores)\n",
        "\n",
        "        # --- save analysis\n",
        "        with open(\"analysis_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(analysis, f, indent=2, ensure_ascii=False)\n",
        "        print(\"\\nFull analysis written to analysis_output.json\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during processing: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up temporary file\n",
        "        if 'temp_path' in locals() and os.path.exists(temp_path):\n",
        "            try:\n",
        "                os.unlink(temp_path)\n",
        "                print(\"Temporary file removed\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Running domain detector on https://drive.google.com/uc?export=download&id=1JhsgpIulCv8Q9NPTZGhrz5-y_RUufMoO with max_cycles=5\n",
            "Successfully read CSV: 3117 rows, 19 columns\n",
            "Data profile built successfully\n",
            "Starting analysis with max_cycles=5\n",
            "Decision point ‚Äì iteration 1, scores: {'correctness': 4, 'relevance': 3, 'coverage': 3, 'insightfulness': 4, 'novelty': 4}\n",
            "üîÑ Analysis needs improvement (iteration 1)\n",
            "Decision point ‚Äì iteration 2, scores: {'correctness': 4, 'relevance': 3, 'coverage': 3, 'insightfulness': 4, 'novelty': 4}\n",
            "üîÑ Analysis needs improvement (iteration 2)\n",
            "Decision point ‚Äì iteration 3, scores: {'correctness': 4, 'relevance': 3, 'coverage': 3, 'insightfulness': 4, 'novelty': 4}\n",
            "üõë Reached maximum cycles (3), ending execution.\n",
            "\n",
            "=== DOMAIN DETECTION HISTORY ===\n",
            "\n",
            "üìä ITERATION 2\n",
            "üìå Domain: Customer Relationship Management (CRM)\n",
            "üìà Scores:\n",
            "   correctness    : ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5)\n",
            "   relevance      : ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (3/5)\n",
            "   coverage       : ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ (3/5)\n",
            "   insightfulness : ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5)\n",
            "   novelty        : ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ (4/5)\n",
            "üìù Analysis: The dataset provides a comprehensive view of customer interactions and characteristics within a CRM context, focusing on...\n",
            "\n",
            "=== FINAL RESULTS ===\n",
            "‚úÖ Domain: Customer Relationship Management (CRM)\n",
            "‚úÖ Core Concepts: Customer Lifetime Value (CLV), Customer Acquisition, Customer Retention, Revenue Segmentation, Customer Segmentation, Churn Analysis\n",
            "\n",
            "üìä DESCRIPTIVE ANALYSIS:\n",
            "The dataset provides a comprehensive view of customer interactions and characteristics within a CRM context, focusing on a specific period (Period 9). It includes 3,117 unique customers, each with detailed attributes such as acquisition channel, sector, and region. The data reveals that most customers have been active for a short duration, with an average of approximately 2 periods, indicating a potential challenge in customer retention. The revenue distribution is skewed, with a significant portion of customers generating lower revenue, while a few contribute to high revenue figures, as evidenced by the high standard deviation. This suggests a need for targeted strategies to enhance revenue from lower-performing segments.\n",
            "\n",
            "üîÆ PREDICTIVE ANALYSIS:\n",
            "Predictive analysis could focus on identifying factors that influence customer retention and CLV. Given the high variance in Expected CLV, machine learning models could be employed to predict CLV based on features like acquisition channel, company age, and contract types. Additionally, the dataset's churn analysis potential is significant, as it includes both current and terminated customers. By analyzing patterns in the 'Period Terminated' and 'Periods Active' fields, predictive models could forecast churn risk, allowing for proactive retention strategies.\n",
            "\n",
            "üåê DOMAIN-RELATED ANALYSIS:\n",
            "In the CRM domain, understanding the interplay between acquisition channels and customer retention is crucial. The dataset indicates that external channels are predominant, which may impact retention rates. Companies could benefit from exploring internal channels or enhancing the onboarding process for externally acquired customers to improve retention. Furthermore, the segmentation by region and company size offers insights into market-specific strategies. For instance, tailoring offerings to the unique needs of different regions or company sizes could optimize customer satisfaction and loyalty, ultimately driving growth in CLV.\n"
          ]
        }
      ],
      "source": [
        "# This script shows how to call `run_domain_detector`\n",
        "# and save / print the results with detailed history visualization\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. Make sure Python can find your agent package / module\n",
        "#    (adapt the path if your repo layout is different)\n",
        "# ------------------------------------------------------------------\n",
        "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
        "sys.path.append(ROOT_DIR)\n",
        "\n",
        "from agents.domain_detector_wrap import run_domain_detector   # noqa: E402\n",
        "\n",
        "def visualize_history(result):\n",
        "    \"\"\"\n",
        "    Visualize the domain detection history across iterations.\n",
        "    \n",
        "    Args:\n",
        "        result: The result from run_domain_detector\n",
        "    \"\"\"\n",
        "    if not result or \"history\" not in result or not result[\"history\"]:\n",
        "        print(\"No history data available\")\n",
        "        return\n",
        "    \n",
        "    # Convert history to DataFrame for easier manipulation\n",
        "    history_df = pd.DataFrame(result[\"history\"])\n",
        "    \n",
        "    # Print a nice tabular summary\n",
        "    print(\"\\n=== DOMAIN DETECTION HISTORY ===\")\n",
        "    \n",
        "    for i, entry in enumerate(result[\"history\"]):\n",
        "        iteration = entry.get(\"iteration\", i)\n",
        "        print(f\"\\nüìä ITERATION {iteration}\")\n",
        "        print(f\"üìå Domain: {entry.get('domain', 'Unknown')}\")\n",
        "        \n",
        "        # Format scores if they exist\n",
        "        scores = entry.get(\"scores\", {})\n",
        "        if scores:\n",
        "            print(\"üìà Scores:\")\n",
        "            for score_name, score_value in scores.items():\n",
        "                stars = \"‚òÖ\" * score_value + \"‚òÜ\" * (5 - score_value)\n",
        "                print(f\"   {score_name.ljust(15)}: {stars} ({score_value}/5)\")\n",
        "                \n",
        "        # Print analysis snippet\n",
        "        analysis_head = entry.get(\"analysis_head\", \"\")\n",
        "        if analysis_head:\n",
        "            print(f\"üìù Analysis: {analysis_head}...\")\n",
        "            \n",
        "    # Plot score evolution if more than one iteration with scores\n",
        "    scored_entries = [entry for entry in result[\"history\"] if entry.get(\"scores\")]\n",
        "    if len(scored_entries) > 1:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Prepare data for plotting\n",
        "        iterations = []\n",
        "        score_data = {}\n",
        "        \n",
        "        for entry in scored_entries:\n",
        "            iter_num = entry.get(\"iteration\", 0)\n",
        "            iterations.append(iter_num)\n",
        "            \n",
        "            for score_name, score_value in entry.get(\"scores\", {}).items():\n",
        "                if score_name not in score_data:\n",
        "                    score_data[score_name] = []\n",
        "                score_data[score_name].append(score_value)\n",
        "        \n",
        "        # Plot each score metric\n",
        "        for score_name, values in score_data.items():\n",
        "            plt.plot(iterations, values, marker='o', label=score_name)\n",
        "            \n",
        "        plt.title(\"Evolution of Scores Across Iterations\")\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Score (0-5)\")\n",
        "        plt.ylim(0, 5.5)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def run_and_visualize(csv_path, max_cycles=5):\n",
        "    \"\"\"\n",
        "    Run domain detector and visualize results with history.\n",
        "    \n",
        "    Args:\n",
        "        csv_path: Path to the CSV file\n",
        "        max_cycles: Maximum number of improvement cycles\n",
        "    \n",
        "    Returns:\n",
        "        The complete result from the domain detector\n",
        "    \"\"\"\n",
        "    print(f\"üöÄ Running domain detector on {csv_path} with max_cycles={max_cycles}\")\n",
        "    \n",
        "    # Run the domain detector\n",
        "    result = run_domain_detector(csv_path, max_cycles)\n",
        "    \n",
        "    # Visualize history\n",
        "    visualize_history(result)\n",
        "    \n",
        "    # Print final results\n",
        "    if \"analysis\" in result:\n",
        "        analysis = result[\"analysis\"]\n",
        "        print(\"\\n=== FINAL RESULTS ===\")\n",
        "        print(f\"‚úÖ Domain: {analysis.get('domain', 'Unknown')}\")\n",
        "        print(f\"‚úÖ Core Concepts: {', '.join(analysis.get('core_concepts', []))}\")\n",
        "        \n",
        "        if \"analysis\" in analysis:\n",
        "            print(\"\\nüìä DESCRIPTIVE ANALYSIS:\")\n",
        "            print(analysis[\"analysis\"].get(\"descriptive\", \"\"))\n",
        "            \n",
        "            print(\"\\nüîÆ PREDICTIVE ANALYSIS:\")\n",
        "            print(analysis[\"analysis\"].get(\"predictive\", \"\"))\n",
        "            \n",
        "            print(\"\\nüåê DOMAIN-RELATED ANALYSIS:\")\n",
        "            print(analysis[\"analysis\"].get(\"domain_related\", \"\"))\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "result = run_and_visualize(\"https://drive.google.com/uc?export=download&id=1JhsgpIulCv8Q9NPTZGhrz5-y_RUufMoO\", max_cycles=5)\n",
        "\n",
        "# To save results to a file:\n",
        "# with open(\"domain_analysis_results.json\", \"w\") as f:\n",
        "#     json.dump(result, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
